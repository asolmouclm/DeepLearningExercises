{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Exercise-Covid_Classification.ipynb","provenance":[{"file_id":"https://github.com/asolmouclm/IADS_SC_2022_DeepLearning/blob/main/Exercise-Day2_Covid_Classification.ipynb","timestamp":1658931042966}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FluT1M9QvcvV","outputId":"c097aaa1-0b06-42c5-af6b-b2cc639f9d05","executionInfo":{"status":"ok","timestamp":1658936405042,"user_tz":-60,"elapsed":284850,"user":{"displayName":"Antonio Santos","userId":"01809173777031446341"}}},"source":["!git clone https://github.com/UCSD-AI4H/COVID-CT.git"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'COVID-CT'...\n","remote: Enumerating objects: 5463, done.\u001b[K\n","remote: Counting objects: 100% (4/4), done.\u001b[K\n","remote: Compressing objects: 100% (4/4), done.\u001b[K\n","remote: Total 5463 (delta 0), reused 1 (delta 0), pack-reused 5459\u001b[K\n","Receiving objects: 100% (5463/5463), 1.09 GiB | 4.02 MiB/s, done.\n","Resolving deltas: 100% (360/360), done.\n","Checking out files: 100% (1048/1048), done.\n"]}]},{"cell_type":"code","metadata":{"id":"hbh1Nm2mv_cF"},"source":["## unzip archives  q - quite, n - don't overwrite\n","!unzip -q -n '/content/COVID-CT/Images-processed/CT_COVID.zip'\n","!unzip -q -n '/content/COVID-CT/Images-processed/CT_NonCOVID.zip'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7gV196flwWAH","outputId":"3d4c4e4a-0814-4660-acf9-4c6c7e4d6604","executionInfo":{"status":"ok","timestamp":1658936406195,"user_tz":-60,"elapsed":16,"user":{"displayName":"Antonio Santos","userId":"01809173777031446341"}}},"source":["import os\n","path='/content/CT_COVID/'\n","for count, filename in enumerate(os.listdir(path)): \n","    dst =\"covid-\" + str(count) + \".png\"\n","    src =path+ filename \n","    dst =path+ dst \n","        \n","    # rename() function will \n","    # rename all the files \n","    os.rename(src, dst)\n","print(\"Covid Positive cases: \", count)\n","\n","path='/content/CT_NonCOVID/'\n","for count, filename in enumerate(os.listdir(path)): \n","    dst =\"noncovid-\" + str(count) + \".png\"\n","    src =path+ filename \n","    dst =path+ dst \n","        \n","    # rename() function will \n","    # rename all the files \n","    os.rename(src, dst)\n","\n","print(\"Covid Negative cases: \", count)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Covid Positive cases:  348\n","Covid Negative cases:  396\n"]}]},{"cell_type":"markdown","source":["## Exercise\n","We have downlaoded the COVID Image Data for you. Now, please a train and build a classification model to detect COVID vs Non-Covid cases. "],"metadata":{"id":"wpGcwbg9T4UO"}},{"cell_type":"code","source":["# Make new base directory\n","positive_dataset_dir = '/content/CT_COVID'\n","negative_dataset_dir = '/content/CT_NonCOVID'\n","base_dir = '/content/COVID_small'\n","os.mkdir(base_dir)"],"metadata":{"id":"dp2Xq3qJwD6c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dir = os.path.join(base_dir, 'train')\n","os.mkdir(train_dir)\n","\n","validation_dir = os.path.join(base_dir, 'validation')\n","os.mkdir(validation_dir)\n","\n","test_dir = os.path.join(base_dir, 'test')\n","os.mkdir(test_dir)\n","\n","train_pos_dir = os.path.join(train_dir, 'positive')\n","os.mkdir(train_pos_dir)\n","\n","train_neg_dir = os.path.join(train_dir, 'negative')\n","os.mkdir(train_neg_dir)\n","\n","validation_pos_dir = os.path.join(validation_dir, 'positive')\n","os.mkdir(validation_pos_dir)\n","\n","validation_neg_dir = os.path.join(validation_dir, 'negative')\n","os.mkdir(validation_neg_dir)\n","\n","test_pos_dir = os.path.join(test_dir, 'positive')\n","os.mkdir(test_pos_dir)\n","\n","test_neg_dir = os.path.join(test_dir, 'negative')\n","os.mkdir(test_neg_dir)"],"metadata":{"id":"y0WDxyyWxYza"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shutil\n","fnames = ['covid-{}.png'.format(i) for i in range(250)]\n","for fname in fnames:\n","    src = os.path.join(positive_dataset_dir, fname)\n","    dst = os.path.join(train_pos_dir, fname)\n","    #print(src,dst)\n","    shutil.copyfile(src, dst)\n","\n","fnames = ['covid-{}.png'.format(i) for i in range(250, 300)]\n","for fname in fnames:\n","    src = os.path.join(positive_dataset_dir, fname)\n","    dst = os.path.join(validation_pos_dir, fname)\n","    #print(src,dst)\n","    shutil.copyfile(src, dst)\n","\n","fnames = ['covid-{}.png'.format(i) for i in range(300, 348)]\n","for fname in fnames:\n","    src = os.path.join(positive_dataset_dir, fname)\n","    dst = os.path.join(test_pos_dir, fname)\n","    #print(src,dst)\n","    shutil.copyfile(src, dst)    \n","\n","fnames = ['noncovid-{}.png'.format(i) for i in range(300)]\n","for fname in fnames:\n","    src = os.path.join(negative_dataset_dir, fname)\n","    dst = os.path.join(train_neg_dir, fname)\n","    #print(src,dst)\n","    shutil.copyfile(src, dst)\n","\n","fnames = ['noncovid-{}.png'.format(i) for i in range(300, 350)]\n","for fname in fnames:\n","    src = os.path.join(negative_dataset_dir, fname)\n","    dst = os.path.join(validation_neg_dir, fname)\n","    #print(src,dst)\n","    shutil.copyfile(src, dst)\n","\n","fnames = ['noncovid-{}.png'.format(i) for i in range(350, 396)]\n","for fname in fnames:\n","    src = os.path.join(negative_dataset_dir, fname)\n","    dst = os.path.join(test_neg_dir, fname)\n","    #print(src,dst)\n","    shutil.copyfile(src, dst)"],"metadata":{"id":"CMaBajWHzzUy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('total training positive images:', len(os.listdir(train_pos_dir)))\n","print('total training negative images:', len(os.listdir(train_neg_dir)))\n","\n","print('total validation positive images:', len(os.listdir(validation_pos_dir)))\n","print('total validation negative images:', len(os.listdir(validation_neg_dir)))\n","\n","print('total test positive images:', len(os.listdir(test_pos_dir)))\n","print('total test negative images:', len(os.listdir(test_neg_dir)))"],"metadata":{"id":"S8u_gwME1wNR","executionInfo":{"status":"ok","timestamp":1658936406557,"user_tz":-60,"elapsed":5,"user":{"displayName":"Antonio Santos","userId":"01809173777031446341"}},"outputId":"067853a9-2ac3-4800-e746-fec0dc9803ad","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total training positive images: 250\n","total training negative images: 300\n","total validation positive images: 50\n","total validation negative images: 50\n","total test positive images: 48\n","total test negative images: 46\n"]}]},{"cell_type":"code","source":["from keras import layers\n","from keras import models\n","from tensorflow.keras import optimizers\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='relu',\n","input_shape=(150, 150, 3)))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Flatten())\n","#model.add(layers.Dropout(0.5)) # Dropout\n","model.add(layers.Dense(512, activation='relu'))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer=optimizers.RMSprop(lr=1e-4),\n","              metrics=['acc'])"],"metadata":{"id":"akwlRQqq2MEB","executionInfo":{"status":"ok","timestamp":1658936412539,"user_tz":-60,"elapsed":5984,"user":{"displayName":"Antonio Santos","userId":"01809173777031446341"}},"outputId":"3f278197-a754-4d7a-e8f3-7d26367c3edc","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(RMSprop, self).__init__(name, **kwargs)\n"]}]},{"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(train_dir,\n","                                                    target_size=(150, 150), \n","                                                    batch_size=10,\n","                                                    class_mode='binary')\n","\n","validation_generator = test_datagen.flow_from_directory(validation_dir,\n","                                                        target_size=(150, 150),\n","                                                        batch_size=15,\n","                                                        class_mode='binary')\n","history = model.fit_generator(train_generator,\n","                              steps_per_epoch=30,\n","                              epochs=7,\n","                              validation_data=validation_generator,\n","                              validation_steps=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDTik_nbRaHD","executionInfo":{"status":"ok","timestamp":1658936526155,"user_tz":-60,"elapsed":17033,"user":{"displayName":"Antonio Santos","userId":"01809173777031446341"}},"outputId":"8979a7f2-839f-40c4-a0ce-46f20c45cdc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 550 images belonging to 2 classes.\n","Found 100 images belonging to 2 classes.\n","Epoch 1/7\n"," 1/30 [>.............................] - ETA: 1s - loss: 0.5328 - acc: 0.9000"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"]},{"output_type":"stream","name":"stdout","text":["30/30 [==============================] - 2s 63ms/step - loss: 0.3432 - acc: 0.8467 - val_loss: 0.5417 - val_acc: 0.7467\n","Epoch 2/7\n","30/30 [==============================] - 2s 57ms/step - loss: 0.3506 - acc: 0.8633 - val_loss: 0.5780 - val_acc: 0.7467\n","Epoch 3/7\n","30/30 [==============================] - 2s 63ms/step - loss: 0.3351 - acc: 0.8700 - val_loss: 0.4779 - val_acc: 0.7600\n","Epoch 4/7\n","30/30 [==============================] - 2s 66ms/step - loss: 0.2784 - acc: 0.8933 - val_loss: 0.5652 - val_acc: 0.7467\n","Epoch 5/7\n","30/30 [==============================] - 2s 67ms/step - loss: 0.2945 - acc: 0.8800 - val_loss: 0.5659 - val_acc: 0.7733\n","Epoch 6/7\n","30/30 [==============================] - 2s 60ms/step - loss: 0.2935 - acc: 0.9000 - val_loss: 0.5235 - val_acc: 0.8133\n","Epoch 7/7\n","30/30 [==============================] - 2s 65ms/step - loss: 0.2526 - acc: 0.9067 - val_loss: 0.5871 - val_acc: 0.7467\n"]}]},{"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(train_dir,\n","                                                    target_size=(150, 150), \n","                                                    batch_size=10,\n","                                                    class_mode='binary')\n","\n","validation_generator = test_datagen.flow_from_directory(validation_dir,\n","                                                        target_size=(150, 150),\n","                                                        batch_size=20,\n","                                                        class_mode='binary')\n","\n","history = model.fit_generator(train_generator,\n","                              steps_per_epoch=30,\n","                              epochs=7,\n","                              validation_data=validation_generator,\n","                              validation_steps=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H2cQ1-lgG84d","executionInfo":{"status":"ok","timestamp":1658935785707,"user_tz":-60,"elapsed":16565,"user":{"displayName":"Antonio Santos","userId":"01809173777031446341"}},"outputId":"17b59d26-5807-4079-dc67-dc5cc743b310"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 550 images belonging to 2 classes.\n","Found 100 images belonging to 2 classes.\n","Epoch 1/7\n"," 1/30 [>.............................] - ETA: 1s - loss: 0.4797 - acc: 0.7000"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"]},{"output_type":"stream","name":"stdout","text":["30/30 [==============================] - 2s 70ms/step - loss: 0.6473 - acc: 0.6133 - val_loss: 0.6634 - val_acc: 0.5800\n","Epoch 2/7\n","30/30 [==============================] - 2s 67ms/step - loss: 0.6433 - acc: 0.6733 - val_loss: 0.6670 - val_acc: 0.5700\n","Epoch 3/7\n","30/30 [==============================] - 2s 62ms/step - loss: 0.6284 - acc: 0.6200 - val_loss: 0.6460 - val_acc: 0.6500\n","Epoch 4/7\n","30/30 [==============================] - 2s 64ms/step - loss: 0.6034 - acc: 0.6767 - val_loss: 0.6287 - val_acc: 0.5800\n","Epoch 5/7\n","30/30 [==============================] - 2s 67ms/step - loss: 0.5832 - acc: 0.6767 - val_loss: 0.6082 - val_acc: 0.6400\n","Epoch 6/7\n","30/30 [==============================] - 2s 62ms/step - loss: 0.5498 - acc: 0.7233 - val_loss: 0.5964 - val_acc: 0.6600\n","Epoch 7/7\n","30/30 [==============================] - 2s 63ms/step - loss: 0.5663 - acc: 0.7067 - val_loss: 0.5812 - val_acc: 0.6800\n"]}]}]}